{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea2fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac39f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\AppData\\Local\\Temp\\ipykernel_50896\\4283496048.py:35: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  sim = doc.similarity(choice)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df1: 78061\n",
      "Length of results: 78061\n",
      "              itemcode     itemgroup                   grpdes  servicenature  \\\n",
      "0      1125S0000009163          1125                Earthwork        23847.0   \n",
      "1      1125S0000009164          1125                Earthwork            NaN   \n",
      "2      1125S0000009169          1125                Earthwork            NaN   \n",
      "3      1125S0000009171          1125                Earthwork            NaN   \n",
      "4      1125S0000009173          1125                Earthwork            NaN   \n",
      "...                ...           ...                      ...            ...   \n",
      "78056  4020S0000020022          4020      Others (Controlled)        23787.0   \n",
      "78057  4020S0000020027          4020      Others (Controlled)        23787.0   \n",
      "78058  4020S0000020028          4020      Others (Controlled)        24227.0   \n",
      "78059  6E11S0018000055  600000000000  General Electrical Work        23847.0   \n",
      "78060  6E11S0018000056  600000000000  General Electrical Work        23847.0   \n",
      "\n",
      "       gstrate  maxyr    wovalue  \\\n",
      "0          0.0   2017  497944.95   \n",
      "1          NaN   2016  138317.75   \n",
      "2          NaN   2015  755129.55   \n",
      "3          NaN   2015   58660.00   \n",
      "4          NaN   2015   13732.25   \n",
      "...        ...    ...        ...   \n",
      "78056     18.0   2016  535975.00   \n",
      "78057     18.0   2016   56940.00   \n",
      "78058      0.0   2015  339584.40   \n",
      "78059     18.0   2017    1200.00   \n",
      "78060     18.0   2017     975.00   \n",
      "\n",
      "                                                itemdesc  SACcode isactive  \\\n",
      "0      SCOPE :- As described further; DETAILS :- Labo...      NaN        Y   \n",
      "1      Labour Charges Watering, Rolling & Compaction ...      NaN        Y   \n",
      "2      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "3      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "4      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "...                                                  ...      ...      ...   \n",
      "78056  SCOPE :- As described further; DETAILS :- Char...      NaN        Y   \n",
      "78057  SCOPE :- As described further; DETAILS :- Surf...      NaN        Y   \n",
      "78058  SCOPE :- As described further; DETAILS :- R.C....      NaN        Y   \n",
      "78059  SCOPE :- Electrical wiring; SCOPE INCLUDES :- ...      NaN        Y   \n",
      "78060  SCOPE :- Electrical wiring; SCOPE INCLUDES :- ...      NaN        Y   \n",
      "\n",
      "                              Matched_ServiceDescription  Matched_ServiceCode  \n",
      "0      Construction services of industrial buildings ...             995413.0  \n",
      "1      General construction services of long-distance...             995423.0  \n",
      "2      General construction services of long-distance...             995423.0  \n",
      "3      Services involving Repair, alterations, additi...             995479.0  \n",
      "4                                                   None                  NaN  \n",
      "...                                                  ...                  ...  \n",
      "78056                                               None                  NaN  \n",
      "78057  Cosmetic treatment (including cosmetic/plastic...             999722.0  \n",
      "78058                                               None                  NaN  \n",
      "78059  General construction services of long-distance...             995423.0  \n",
      "78060  General construction services of long-distance...             995423.0  \n",
      "\n",
      "[78061 rows x 12 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "df1 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='Service items')\n",
    "df2 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='SAC STD')\n",
    "\n",
    "\n",
    "column_df1 = 'itemdesc'\n",
    "column_df2 = 'SAC Description'\n",
    "code_column_df2 = 'SAC Code'\n",
    "\n",
    "df1[column_df1] = df1[column_df1].astype(str).fillna('')\n",
    "df2[column_df2] = df2[column_df2].astype(str).fillna('')\n",
    "\n",
    "\n",
    "def text_to_spacy_doc(text):\n",
    "    if isinstance(text, str):\n",
    "        return nlp(text)\n",
    "    return nlp('')\n",
    "\n",
    "\n",
    "df1['spacy_doc'] = df1[column_df1].apply(text_to_spacy_doc)\n",
    "df2['spacy_doc'] = df2[column_df2].apply(text_to_spacy_doc)\n",
    "\n",
    "\n",
    "def match_texts(doc, choices, codes, threshold=0.80):\n",
    "    similarities = []\n",
    "    for choice, code in zip(choices, codes):\n",
    "        if doc and choice:\n",
    "            sim = doc.similarity(choice)\n",
    "            similarities.append((choice, sim, code))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if similarities:\n",
    "        best_match, best_score, best_code = similarities[0]\n",
    "        return (best_match.text, best_code) if best_score >= threshold else (None, None)\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "choices = df2['spacy_doc'].tolist()\n",
    "codes = df2[code_column_df2].tolist()\n",
    "\n",
    "\n",
    "results = df1['spacy_doc'].apply(lambda doc: match_texts(doc, choices, codes, threshold=0.75))\n",
    "\n",
    "\n",
    "print(f\"Length of df1: {len(df1)}\")\n",
    "print(f\"Length of results: {len(results)}\")\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    if result is None or len(result) != 2:\n",
    "        print(\"Unexpected result format:\", result)\n",
    "\n",
    "\n",
    "if len(results) == len(df1):\n",
    "    df1['Matched_ServiceDescription'], df1['Matched_ServiceCode'] = zip(*results)\n",
    "else:\n",
    "    print(\"Error: Length of results does not match length of df1\")\n",
    "\n",
    "\n",
    "df1.drop(columns=['spacy_doc'], inplace=True)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\n",
    "df1 = df1.applymap(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "print(df1)\n",
    "\n",
    "\n",
    "df1.to_excel('matched_results.xlsx',encoding='utf-8',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04e9379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\vigne\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\vigne\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4fcdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df1: 78061\n",
      "Length of results: 78061\n",
      "              itemcode     itemgroup                   grpdes  servicenature  \\\n",
      "0      1125S0000009163          1125                Earthwork        23847.0   \n",
      "1      1125S0000009164          1125                Earthwork            NaN   \n",
      "2      1125S0000009169          1125                Earthwork            NaN   \n",
      "3      1125S0000009171          1125                Earthwork            NaN   \n",
      "4      1125S0000009173          1125                Earthwork            NaN   \n",
      "...                ...           ...                      ...            ...   \n",
      "78056  4020S0000020022          4020      Others (Controlled)        23787.0   \n",
      "78057  4020S0000020027          4020      Others (Controlled)        23787.0   \n",
      "78058  4020S0000020028          4020      Others (Controlled)        24227.0   \n",
      "78059  6E11S0018000055  600000000000  General Electrical Work        23847.0   \n",
      "78060  6E11S0018000056  600000000000  General Electrical Work        23847.0   \n",
      "\n",
      "       gstrate  maxyr    wovalue  \\\n",
      "0          0.0   2017  497944.95   \n",
      "1          NaN   2016  138317.75   \n",
      "2          NaN   2015  755129.55   \n",
      "3          NaN   2015   58660.00   \n",
      "4          NaN   2015   13732.25   \n",
      "...        ...    ...        ...   \n",
      "78056     18.0   2016  535975.00   \n",
      "78057     18.0   2016   56940.00   \n",
      "78058      0.0   2015  339584.40   \n",
      "78059     18.0   2017    1200.00   \n",
      "78060     18.0   2017     975.00   \n",
      "\n",
      "                                                itemdesc  SACcode isactive  \\\n",
      "0      SCOPE :- As described further; DETAILS :- Labo...      NaN        Y   \n",
      "1      Labour Charges Watering, Rolling & Compaction ...      NaN        Y   \n",
      "2      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "3      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "4      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "...                                                  ...      ...      ...   \n",
      "78056  SCOPE :- As described further; DETAILS :- Char...      NaN        Y   \n",
      "78057  SCOPE :- As described further; DETAILS :- Surf...      NaN        Y   \n",
      "78058  SCOPE :- As described further; DETAILS :- R.C....      NaN        Y   \n",
      "78059  SCOPE :- Electrical wiring; SCOPE INCLUDES :- ...      NaN        Y   \n",
      "78060  SCOPE :- Electrical wiring; SCOPE INCLUDES :- ...      NaN        Y   \n",
      "\n",
      "                              Matched_ServiceDescription  Matched_ServiceCode  \n",
      "0      Construction services of industrial buildings ...             995413.0  \n",
      "1      General construction services of long-distance...             995423.0  \n",
      "2      General construction services of long-distance...             995423.0  \n",
      "3      Services involving Repair, alterations, additi...             995479.0  \n",
      "4                                                   None                  NaN  \n",
      "...                                                  ...                  ...  \n",
      "78056                                               None                  NaN  \n",
      "78057  Cosmetic treatment (including cosmetic/plastic...             999722.0  \n",
      "78058                                               None                  NaN  \n",
      "78059  General construction services of long-distance...             995423.0  \n",
      "78060  General construction services of long-distance...             995423.0  \n",
      "\n",
      "[78061 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Code using NLP Spacy\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "df1 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='Service items')\n",
    "df2 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='SAC STD')\n",
    "\n",
    "\n",
    "column_df1 = 'itemdesc'\n",
    "column_df2 = 'SAC Description'\n",
    "code_column_df2 = 'SAC Code'\n",
    "\n",
    "\n",
    "df1[column_df1] = df1[column_df1].astype(str).fillna('')\n",
    "df2[column_df2] = df2[column_df2].astype(str).fillna('')\n",
    "\n",
    "\n",
    "def text_to_spacy_doc(text):\n",
    "    if isinstance(text, str):\n",
    "        return nlp(text)\n",
    "    return nlp('')\n",
    "\n",
    "\n",
    "df1['spacy_doc'] = df1[column_df1].apply(text_to_spacy_doc)\n",
    "df2['spacy_doc'] = df2[column_df2].apply(text_to_spacy_doc)\n",
    "\n",
    "\n",
    "def match_texts(doc, choices, codes, threshold=0.80):\n",
    "    similarities = []\n",
    "    for choice, code in zip(choices, codes):\n",
    "        if doc and choice and doc.has_vector and choice.has_vector:\n",
    "            sim = doc.similarity(choice)\n",
    "            similarities.append((choice, sim, code))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if similarities:\n",
    "        best_match, best_score, best_code = similarities[0]\n",
    "        return (best_match.text, best_code) if best_score >= threshold else (None, None)\n",
    "    return(None,None)\n",
    "\n",
    "choices = df2['spacy_doc'].tolist()\n",
    "codes = df2[code_column_df2].tolist()\n",
    "\n",
    "\n",
    "results = df1['spacy_doc'].apply(lambda doc: match_texts(doc, choices, codes, threshold=0.80))\n",
    "\n",
    "print(f\"Length of df1: {len(df1)}\")\n",
    "print(f\"Length of results: {len(results)}\")\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    if result is None or len(result) != 2:\n",
    "        print(\"Unexpected result format:\", result)\n",
    "\n",
    "\n",
    "if len(results) == len(df1):\n",
    "    df1['Matched_ServiceDescription'], df1['Matched_ServiceCode'] = zip(*results)\n",
    "else:\n",
    "    print(\"Error: Length of results does not match length of df1\")\n",
    "\n",
    "\n",
    "df1.drop(columns=['spacy_doc'], inplace=True)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "df1 = df1.applymap(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "df1.to_csv('matched_results.csv', index=False)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "674ebd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "d = pd.read_csv('matched_results.csv')\n",
    "with open('data.pkl', 'wb') as file:\n",
    "        pickle.dump(d, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27943e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\AppData\\Local\\Temp\\ipykernel_49180\\241768089.py:36: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  return [doc.similarity(choice) for choice in choices]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train_dropna, y_train_dropna)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "df1 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='Service items')\n",
    "df2 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='SAC STD')\n",
    "\n",
    "\n",
    "column_df1 = 'itemdesc'\n",
    "column_df2 = 'SAC Description'\n",
    "code_column_df2 = 'SAC Code'\n",
    "\n",
    "\n",
    "df1[column_df1] = df1[column_df1].astype(str).fillna('')\n",
    "df2[column_df2] = df2[column_df2].astype(str).fillna('')\n",
    "\n",
    "df1['spacy_doc'] = df1[column_df1].apply(lambda text: nlp(text) if isinstance(text, str) else nlp(''))\n",
    "df2['spacy_doc'] = df2[column_df2].apply(lambda text: nlp(text) if isinstance(text, str) else nlp(''))\n",
    "\n",
    "choices = df2['spacy_doc'].tolist()\n",
    "codes = df2[code_column_df2].tolist()\n",
    "\n",
    "\n",
    "def calculate_similarity(doc, choices):\n",
    "    return [doc.similarity(choice) for choice in choices]\n",
    "\n",
    "\n",
    "df1['similarities'] = df1['spacy_doc'].apply(lambda doc: calculate_similarity(doc, choices))\n",
    "\n",
    "\n",
    "similarity_df = pd.DataFrame(df1['similarities'].tolist(), index=df1.index, columns=[f'sim_{i}' for i in range(len(choices))])\n",
    "\n",
    "\n",
    "df1 = pd.concat([df1, similarity_df], axis=1)\n",
    "\n",
    "\n",
    "df2['is_match'] = 1  \n",
    "df1['is_match'] = 0 \n",
    "\n",
    "\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "\n",
    "features = [f'sim_{i}' for i in range(len(choices))]\n",
    "target = 'is_match'\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df[features], combined_df[target], test_size=0.2, random_state=42)\n",
    "                                                    \n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "X_train_dropna = X_train.dropna()\n",
    "y_train_dropna = y_train.loc[X_train_dropna.index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train_dropna, y_train_dropna)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_dropna, y_train_dropna)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "df1['is_match_pred'] = clf.predict(df1[features])\n",
    "\n",
    "df1['Matched_ServiceDescription'] = df1.apply(lambda row: df2.loc[choices.index(max(choices, key=lambda x: row['spacy_doc'].similarity(x))), column_df2] if row['is_match_pred'] else None, axis=1)\n",
    "df1['Matched_ServiceCode'] = df1.apply(lambda row: df2.loc[choices.index(max(choices, key=lambda x: row['spacy_doc'].similarity(x))), code_column_df2] if row['is_match_pred'] else None, axis=1)\n",
    "\n",
    "\n",
    "df1.drop(columns=['spacy_doc', 'similarities'] + features, inplace=True)\n",
    "\n",
    "\n",
    "df1.to_csv('matched_results_ml.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print(\"Matching process completed and results saved to 'matched_results_ml.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6cf54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\AppData\\Local\\Temp\\ipykernel_49180\\1505964970.py:34: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  return [doc.similarity(choice) for choice in choices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15617\n",
      "           1       1.00      1.00      1.00       109\n",
      "\n",
      "    accuracy                           1.00     15726\n",
      "   macro avg       1.00      1.00      1.00     15726\n",
      "weighted avg       1.00      1.00      1.00     15726\n",
      "\n",
      "Matching process completed and results saved to 'matched_results_ml.csv'.\n"
     ]
    }
   ],
   "source": [
    "#Code using Pipeline and classifier\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "df1 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='Service items')\n",
    "df2 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='SAC STD')\n",
    "\n",
    "column_df1 = 'itemdesc'\n",
    "column_df2 = 'SAC Description'\n",
    "code_column_df2 = 'SAC Code'\n",
    "\n",
    "\n",
    "df1[column_df1] = df1[column_df1].astype(str).fillna('')\n",
    "df2[column_df2] = df2[column_df2].astype(str).fillna('')\n",
    "\n",
    "\n",
    "df1['spacy_doc'] = df1[column_df1].apply(lambda text: nlp(text) if isinstance(text, str) else nlp(''))\n",
    "df2['spacy_doc'] = df2[column_df2].apply(lambda text: nlp(text) if isinstance(text, str) else nlp(''))\n",
    "\n",
    "\n",
    "choices = df2['spacy_doc'].tolist()\n",
    "codes = df2[code_column_df2].tolist()\n",
    "\n",
    "\n",
    "def calculate_similarity(doc, choices):\n",
    "    return [doc.similarity(choice) for choice in choices]\n",
    "\n",
    "\n",
    "df1['similarities'] = df1['spacy_doc'].apply(lambda doc: calculate_similarity(doc, choices))\n",
    "\n",
    "\n",
    "similarity_df = pd.DataFrame(df1['similarities'].tolist(), index=df1.index, columns=[f'sim_{i}' for i in range(len(choices))])\n",
    "df1 = pd.concat([df1, similarity_df], axis=1)\n",
    "\n",
    "\n",
    "df2['is_match'] = 1 \n",
    "df1['is_not_match'] = 0 \n",
    "\n",
    "\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "\n",
    "features = [f'sim_{i}' for i in range(len(choices))]\n",
    "target = 'is_match'\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df[features], combined_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "df1['is_match_pred'] = pipeline.predict(df1[features])\n",
    "\n",
    "df1['Matched_ServiceDescription'] = df1.apply(lambda row: df2.loc[choices.index(max(choices, key=lambda x: row['spacy_doc'].similarity(x))), column_df2] if row['is_match_pred'] else None, axis=1)\n",
    "df1['Matched_ServiceCode'] = df1.apply(lambda row: df2.loc[choices.index(max(choices, key=lambda x: row['spacy_doc'].similarity(x))), code_column_df2] if row['is_match_pred'] else None, axis=1)\n",
    "\n",
    "\n",
    "df1.drop(columns=['spacy_doc', 'similarities'] + features, inplace=True)\n",
    "df1.to_csv('matched_results_ml.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print(\"Matching process completed and results saved to 'matched_results_ml.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f29aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              itemcode     itemgroup                   grpdes  servicenature  \\\n",
      "0      1125S0000009163          1125                Earthwork        23847.0   \n",
      "1      1125S0000009164          1125                Earthwork            NaN   \n",
      "2      1125S0000009169          1125                Earthwork            NaN   \n",
      "3      1125S0000009171          1125                Earthwork            NaN   \n",
      "4      1125S0000009173          1125                Earthwork            NaN   \n",
      "...                ...           ...                      ...            ...   \n",
      "78056  4020S0000020022          4020      Others (Controlled)        23787.0   \n",
      "78057  4020S0000020027          4020      Others (Controlled)        23787.0   \n",
      "78058  4020S0000020028          4020      Others (Controlled)        24227.0   \n",
      "78059  6E11S0018000055  600000000000  General Electrical Work        23847.0   \n",
      "78060  6E11S0018000056  600000000000  General Electrical Work        23847.0   \n",
      "\n",
      "       gstrate  maxyr    wovalue  \\\n",
      "0          0.0   2017  497944.95   \n",
      "1          NaN   2016  138317.75   \n",
      "2          NaN   2015  755129.55   \n",
      "3          NaN   2015   58660.00   \n",
      "4          NaN   2015   13732.25   \n",
      "...        ...    ...        ...   \n",
      "78056     18.0   2016  535975.00   \n",
      "78057     18.0   2016   56940.00   \n",
      "78058      0.0   2015  339584.40   \n",
      "78059     18.0   2017    1200.00   \n",
      "78060     18.0   2017     975.00   \n",
      "\n",
      "                                                itemdesc  SACcode isactive  \\\n",
      "0      SCOPE :- As described further; DETAILS :- Labo...      NaN        Y   \n",
      "1      Labour Charges Watering, Rolling & Compaction ...      NaN        Y   \n",
      "2      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "3      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "4      SCOPE :- As described further; DETAILS :- Kodi...      NaN        Y   \n",
      "...                                                  ...      ...      ...   \n",
      "78056  SCOPE :- As described further; DETAILS :- Char...      NaN        Y   \n",
      "78057  SCOPE :- As described further; DETAILS :- Surf...      NaN        Y   \n",
      "78058  SCOPE :- As described further; DETAILS :- R.C....      NaN        Y   \n",
      "78059  SCOPE :- Electrical wiring; SCOPE INCLUDES :- ...      NaN        Y   \n",
      "78060  SCOPE :- Electrical wiring; SCOPE INCLUDES :- ...      NaN        Y   \n",
      "\n",
      "       is_match  is_match_pred Matched_ServiceDescription Matched_ServiceCode  \n",
      "0             0              0                       None                None  \n",
      "1             0              0                       None                None  \n",
      "2             0              0                       None                None  \n",
      "3             0              0                       None                None  \n",
      "4             0              0                       None                None  \n",
      "...         ...            ...                        ...                 ...  \n",
      "78056         0              0                       None                None  \n",
      "78057         0              0                       None                None  \n",
      "78058         0              0                       None                None  \n",
      "78059         0              0                       None                None  \n",
      "78060         0              0                       None                None  \n",
      "\n",
      "[78061 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244b6aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 78\u001b[0m\n\u001b[0;32m     74\u001b[0m choices \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_doc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     75\u001b[0m codes \u001b[38;5;241m=\u001b[39m df2[code_column_df2]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 78\u001b[0m results \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_doc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m doc: match_texts(doc, choices, codes, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.80\u001b[39m))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of df1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 78\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     74\u001b[0m choices \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_doc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     75\u001b[0m codes \u001b[38;5;241m=\u001b[39m df2[code_column_df2]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 78\u001b[0m results \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy_doc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m doc: match_texts(doc, choices, codes, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.80\u001b[39m))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of df1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mmatch_texts\u001b[1;34m(doc, choices, codes, threshold)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m     39\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc] \u001b[38;5;241m+\u001b[39m choices\n\u001b[1;32m---> 42\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\u001b[38;5;241m.\u001b[39mfit_transform(texts)\n\u001b[0;32m     43\u001b[0m vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     46\u001b[0m doc_vector \u001b[38;5;241m=\u001b[39m vectors[\u001b[38;5;241m0\u001b[39m]  \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#Code using Tfidvectorizer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "df1 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='Service items')\n",
    "df2 = pd.read_excel(\"D:\\\\doneee\\\\Service items for SAC Update - Copy.xlsx\", sheet_name='SAC STD')\n",
    "\n",
    "\n",
    "column_df1 = 'itemdesc'\n",
    "column_df2 = 'SAC Description'\n",
    "code_column_df2 = 'SAC Code'\n",
    "\n",
    "\n",
    "df1[column_df1] = df1[column_df1].astype(str).fillna('')\n",
    "df2[column_df2] = df2[column_df2].astype(str).fillna('')\n",
    "\n",
    "\n",
    "def text_to_spacy_doc(text):\n",
    "    if isinstance(text, str):\n",
    "        return nlp(text)\n",
    "    return nlp('')\n",
    "\n",
    "\n",
    "df1['spacy_doc'] = df1[column_df1].apply(text_to_spacy_doc)\n",
    "df2['spacy_doc'] = df2[column_df2].apply(text_to_spacy_doc)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def match_texts(doc, choices, codes, threshold=0.80):\n",
    "    if not doc or not choices:\n",
    "        return []\n",
    "\n",
    "    \n",
    "    texts = [doc] + choices\n",
    "\n",
    "    \n",
    "    vectorizer = TfidfVectorizer().fit_transform(texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    \n",
    "    doc_vector = vectors[0]  \n",
    "    similarities = []\n",
    "    for i, (choice, code) in enumerate(zip(choices, codes)):\n",
    "        choice_vector = vectors[i + 1]  \n",
    "        sim = cosine_similarity([doc_vector], [choice_vector])[0][0]\n",
    "        if sim >= threshold:\n",
    "            similarities.append((choice, sim, code))\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "    doc = \"This is a sample document.\"\n",
    "    choices = [\"This is a sample text.\", \"Another document.\", \"Completely different text.\"]\n",
    "    codes = [\"code1\", \"code2\", \"code3\"]\n",
    "\n",
    "    matched_texts = match_texts(doc, choices, codes, threshold=0.2)\n",
    "    for match in matched_texts:\n",
    "    \n",
    "        print(f\"Choice: {match[0]}, Similarity: {match[1]:.2f}, Code: {match[2]}\")\n",
    "\n",
    "\n",
    "        similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if similarities:\n",
    "            best_match, best_score, best_code = similarities[0]\n",
    "            return (best_match.text, best_code) if best_score >= threshold else (None, None)\n",
    "        return(None,None)\n",
    "\n",
    "choices = df2['spacy_doc'].tolist()\n",
    "codes = df2[code_column_df2].tolist()\n",
    "\n",
    "\n",
    "results = df1['spacy_doc'].apply(lambda doc: match_texts(doc, choices, codes, threshold=0.80))\n",
    "\n",
    "print(f\"Length of df1: {len(df1)}\")\n",
    "print(f\"Length of results: {len(results)}\")\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    if result is None or len(result) != 2:\n",
    "        print(\"Unexpected result format:\", result)\n",
    "\n",
    "\n",
    "if len(results) == len(df1):\n",
    "    df1['Matched_ServiceDescription'], df1['Matched_ServiceCode'] = zip(*results)\n",
    "else:\n",
    "    print(\"Error: Length of results does not match length of df1\")\n",
    "\n",
    "\n",
    "df1.drop(columns=['spacy_doc'], inplace=True)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "df1 = df1.applymap(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "df1.to_csv('matched_results.csv', index=False)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660758a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
